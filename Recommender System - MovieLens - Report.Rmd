---
title: "Recommender System: Personality Theory Insights on the MovieLens Dataset" 
subtitle: "Harvard Data Science Professional Certificate Capstone Project"
author: "Klebert Toscano de S. Cintra"
date: FALSE
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
    toc: true
  tufte::tufte_html:
    toc: true
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
options(knitr.duplicate.label = "allow")
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

```{r, echo=FALSE}
img_path <- "https://github.com/KTSC/Recommender-System---MovieLens/tree/master/img/"
```
# Introduction

## Overview

Businesses in competitive markets are constantly seeking for tools that might be an advantage over competitors. That is often related to the ability to anticipate preferences of their customers and offering products that the client will like, and not regret after. Also typical of competitive markets is the abundance of options to be chosen, and while this might seem like a desirable aspect of customization for the client, research^[This is a well documented psychological phenomenon. To know more read about the [jams experiment](https://psycnet.apa.org/record/2000-16701-012) and this [meta-analysis on choice overload](https://archive-ouverte.unige.ch/unige:76440). Barry Schwartz's book [The Paradox of Choice](https://www.amazon.com/Paradox-Choice-Why-More-Less/dp/149151423X) also covers the subject nicely.] has demonstrated that it makes the decision process more stressful, time consuming and, after the choice has been made, it is perceived as less satisfying. Providing good recommendations can attenuate the problems with cognitive overload, so there is high demand on the market to use data to guide the recommendations. 

Here we try two different approaches to make recommendations using data of a very competitive market: the movie industry. 



## Executive Summary

Among the many methods for recommender systems a very popular one due to its simplicity, interpretability and having low computational demands is the search for a function that best describes the relationship between two or more variables and by doing so, make predictions. This is called a **Linear Model**. The weights of the variables to be combined are then estimated by minimizing the distance between the observed data and the line generated by the function. This is the Least Squares method and the estimates for the weights for the variables are the **Least Squares Estimates (LSE)**. 

```{marginfigure}
The **Least Squares Estimates** method describes how a random variable we want to predict $Y$ is defined by the linear combination of the independent variables $x_i$ given the weights $\beta$ and some random error $\varepsilon$:
$$Y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_i x_i + \varepsilon_i, \, i=1,\dots,N.$$
```


The other method applied here makes use of **Deep Neural Networks**, which are very popular due to their applicability to many types of data. Unlike **LSE**, it can be quite computationally demanding, and not easily interpretable. They are often represented using units (neurons) that are aggregating functions, organized in layers that are able to assign numeric value to different levels of abstraction on the input data. The number of layers constitutes the _depth_ of the neural network.      

```{r, echo = FALSE, out.width = "140%"}
 knitr::include_graphics(file.path(img_path,"nn.png"))
```

`r newthought('The goal')` of this project is to compare these two approaches in a large dataset to create a recommender system. The data used here is the [_MovieLens  Dataset_](https://grouplens.org/datasets/movielens/latest/) consisting of 10 million ratings. The recommender system implemented here is inspired by the famous [Netflix Prize](https://www.netflixprize.com/index.html), which awarded the [winners](http://www.research.att.com/~volinsky/netflix/bpc.html) 1 Million Dollars for getting the **Root Mean Squared Error (RMSE)** of **0.8572**. We will compare different methods using this metric.  

```{r Create test and validation sets, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)


# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
# movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                            title = as.character(title),
#                                            genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
# set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

```{r Loading packages, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
library(ggplot2)
library(h2o)
library(lubridate)
```


`r newthought('The 10M MovieLens dataset')` has ratings by `r length(unique(edx$userId))` unique users for `r length(unique(edx$movieId))` movies. The ratings consist in assigning a number of stars compatible with the appreciation of the user for a particular movie. The first lines of the data frame can be seen below with a short description of variables to the right.  


```{r, echo=FALSE}
knitr::kable(
  head(edx), align = 'c', 
  caption = '**THE VARIABLES**<br/><br/>**userId**: unique identifier for user.<br/><br/>**movieId**: unique identifier for movie<br/><br/>**rating**: how a user rated a particular movie.<br/><br/>**timestamp**: the time when the rating happened in seconds since 01/01/1970.<br/><br/>**title**: the title of the movie.<br/><br/>**genres**: the genres that describe the movie.'
)
```

The procedure proposed here includes the following parts: 

1. Data acquisition and cleaning. Download and partition of the data with 10% of observations for validation and 90% for training of the algorithm. 
2. Exploratory Data Analysis (EDA) and Visualization.  
3. Data transformation for the Linear Models. 
4. Modeling approach 1 - Linear Models.
5. Data transformation for the Deep Neural Network. 
6. Modeling approach 2 - Deep Neural Network. 
7. Evaluation of models and comparison of results. 
8. Conclusion and final considerations. 



```{r, echo = FALSE, fig.fullwidth=TRUE, fig.cap = "Infographic of the framework adopted for this recommender system. Depicts the cyclical nature of the model development that ultimately might go to deployment/production.", warning=FALSE, message=FALSE}
knitr::include_graphics(file.path(img_path,"Data_Science_cycle.png"))
```


# Exploratory Data Analysis (EDA) and Visualization


This report will focus on the relevant visualizations for the methods of choice, and is not an exhaustive assessment of the data. 

We start with the value we want to predict, which is the _rating_ that a user would give to a movie. The ratings range from 0.5 stars to 5 stars in steps of 0.5, resulting in 10 different possible ratings. 

```{r fig-main, echo = FALSE, fig.cap = "Distribution of the ratings for all users in the _edx_ partition, the training set."}
ggplot(data = edx, aes(x = rating)) + 
  geom_bar() + 
  labs(title = "Distribution of Ratings in the Training Set", x = "Rating", y = "Frequency")
```



```{r fig-margin-splittime, echo = FALSE, fig.margin=TRUE, fig.show='hold', fig.cap="Upper plot shows the ratings distribution before timestamp **1045526400** (corresponding to the date 02/18/2003), with granularity of 1. The lower panel shows the distribution after this timestamp, with granularity of 0.5.", fig.width=3.5, fig.height=2.5}

edx_previous <- subset(edx, timestamp < 1045526400)
edx_after <- subset(edx, timestamp >= 1045526400)

# Ratings before 1045526400
plot_previous <- ggplot(data = edx_previous, aes(x = rating)) +
  geom_bar() + 
  scale_fill_manual("#00BEC4") +
  labs(title = "Until Feb 18, 2003", x = "Rating", y = "Frequency")
plot_previous
#...and after 1045526400
plot_after <- ggplot(data = edx_after, aes(x = rating)) +
  geom_bar() + 
  scale_fill_manual("#F8756D") +
  labs(title = "After Feb 18, 2003", x = "Rating", y = "Frequency")
plot_after
```


The _timestamp_ is the count of seconds since the January 01 of 1970. Visual inspection of the dataset shows a change in the way the ratings are distributed. Until February 18 of 2003 (timestamp = 1045526400) the ratings are only integer numbers. After that moment, instead of 5 possible ratings  (1 to 5) there are 10 different possibilities. If we repeat the plot of frequency for ratings with the bars colored based on this particular timestamp we have clear indication this is a relevant aspect to the value we want to predict, and for that reason, this should be taken into consideration. 


```{r fig-main2, echo = FALSE, fig.cap = "Left figure: Distribution of the ratings for all users in the _edx_ partition, before and after February 18 of 2003."}
ggplot(data = edx, aes(x = rating, fill = timestamp > 1045526400)) + 
  geom_bar() + 
  scale_fill_manual(values=c("#00BEC4", "#F8756D")) + 
  labs(title = "Distribution of Ratings split by timestamp 1045526400 (Feb/18/2003)", x = "Rating", y = "Frequency")
```


In addition to this labeling difference dependent on time, we can look for other time-dependent effects. See the picture below and observe the average ratings by month seem to vary slightly. Observe also the clear anomaly in ratings for the early years, showing much higher rating average by month than the remaining time window.  


```{r ratings-time-series, echo = FALSE, fig.cap= "Time series: average of ratings for all users per month", cache=TRUE, message=FALSE, warning=FALSE}

edx_copy <- edx
edx_copy %>% 
  mutate(Month = round_date(as_datetime(timestamp), unit = "month")) %>%
  group_by(Month) %>%
  summarize(Rating = mean(rating)) %>%
  ggplot(aes(Month, Rating)) +
  geom_point() +
  geom_smooth(color="#F8756D") +
  ggtitle("Time series of average rating by month") +
  labs(subtitle = "only the training data is included in this plot.") 
```


As previously shown in _table 1_, numeric values are used as unique identifiers for users and movies, but movies also have a variable for the movie title, which include the year of release between parenthesis. This can be used in future analysis, but for the scope of this project we will not consider characteristics of the movie title or the year it was released. 


The key variable to be examined is the _genres_ column of the dataset. It is a way of describing movies implying a relationship in a higher level of abstraction between movies. It consists of strings with all the genres in which a movie can be classified separated by the character "|". Here we can see the boxplots for the aggregated ratings on all the listed genres. 

```{r genres-boxplots, fig.fullwidth=TRUE, echo = FALSE, eval = TRUE, fig.cap= "Boxplots of ratings grouped by genre.", message=FALSE, warning=FALSE}
library(tidyverse)
edx %>% 
  separate_rows(genres, sep = "\\|") %>% 
  select(genres, rating) %>% 
  group_by(genres) %>%
  ggplot(aes(x = factor(genres), y = rating, fill = genres)) +
  geom_boxplot(show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Movie Genre") +
  ylab("Rating") 
```


There's no obvious difference between genres to make a prediction based on this dimension alone.

To finish this exploratory data analysis we check on the distribution of user rating averages and the average for movies. 


```{r user_and_movie_histograms, echo = FALSE, fig.cap="Histograms of average ratings for user (top) and for movies (bottom).", fig.show='hold', message=FALSE}

edx %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating)) %>%
  filter(n()>= 40) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color =I("#00BEC4")) +
  ggtitle("Histogram of User's average ratings") +
  labs(subtitle = "Average rating") 

edx %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating)) %>%
  filter(n()>= 40) %>%
  ggplot(aes(b_i)) + 
  geom_histogram(bins = 30, color =I("#F8756D")) +
  ggtitle("Histogram of average rating by Movie") +
  labs(subtitle = "Average rating") 
```


# Data Transformation for the Linear Model

The transformations applied to data are based in the following assumptions: 

1. Movie genres provide valuable information about abstract dimensions of the movies. A genre usually have a typical pace of the story, the content of the scenes, saturation of colors, angles for the cameras, emotions provoked, the of casting of actors, and all of those influence the expectations of the audience and might interfere with the ratings even before a movie is watched. So, it could be a less computationally expensive way to numerically describe a movie^[Matrix Factorization using **Principal Component Analysis** and other transformations on this dataset made necessary more than 2000 _principal components_ to explain 90% of the variability on data. Hence, for each prediction on the pairing User-Movie a 2000x2000 vector should be multiplied, and the estimation for 1 million data points would be costly with common personal computers' CPUs.].

2. The evaluation a user provides to a single movie has little generalization for other movies, but the consistent rating of a user for a genre (be it positive or negative) describes the subject^[If an individual shows a consistent preference or response that is stable over time and different contexts is considered a **trait**. Trait psychology and the study of individual differences can be applied to the creation or refinement of recommender systems. The book [Hierarchical Cognitive Models](https://www.cambridge.org/us/academic/subjects/psychology/psychology-research-methods-and-statistics/bayesian-cognitive-modeling-practical-course) has many examples of applications for inferences on latent traits.]. 

3. The more genres a movie has, the more eclectic it might be. Hence, it might attract a larger audience, which result in more ratings, and as a consequence, greater predictive value. 

4. The label to be predicted is not a continuous value ranging from 0 to 5. Instead, they are categories that depend on the timestamp variable, as demonstrated in the pictures above. 



## Genre Transformation for the Linear Model

The proposed procedure is to replicate each line for a "userId" + "movieId" pairing for each unique genre this movie has as attribute. 

Let's use as example the first line of the original dataset. We can see there is one pairing of userId = 1 and movieId = 122:  

```{r, echo=FALSE}
knitr::kable(
  head(edx), align = 'c'
)
```
After the transformation, this pair (userId = 1, movieId = 122) will be featured twice, for having 2 genres for this movie: "Comedy" and "Romance". 




```{r, echo=FALSE, cache=TRUE}

##########################################################
# Make a copy of the original train and test datasets.  
##########################################################

edx_copy <- edx
validation_copy <- validation

##########################################################
# Separate the genres. These lines replicates each ratings for every genre on the movie. 
##########################################################

edx_copy$genres <- lapply(edx_copy$genres, as.character)
validation_copy$genres <- lapply(validation_copy$genres, as.character)
edx_copy <- edx_copy %>% 
  mutate(genre = as.character(genres)) %>%
  separate_rows(genre, sep = "\\|") %>%
  select(-"genres", -"timestamp", -"title")

# Extract the genre in validation datasets
validation_copy <- validation_copy %>% 
  mutate(genre = as.character(genres)) %>%
  separate_rows(genre, sep = "\\|")%>%
  select(-"genres", -"timestamp", -"title")


knitr::kable(
  head(edx_copy), align = 'c'
)
```

The variables _timestamp_ and _title_ were removed as they are not used for this analysis. 

# Modeling approach 1 - Linear Models.

This class of models is based on the premise that the response variable $Y_{u,i}$ representing the rating user $u$ assigned to movie $i$ is equal to a theoretical film rating described by the linear combination of some random variables. The model definition is: 


$Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}$, with $x^k_{u,i} = 1$ if $g_{u,i}$ is genre $k$.

where: 

$\mu$ = The mean of all ratings.

$b_u$ = The user effect. 

$b_i$ = The item effect, in this case, the movie. 

$g_{u,i}$ = Genre for user $u$ rating of movie $i$.

$\varepsilon_{u,i}$ = The error. Randomness on the data, noise in the system. 

$\sum_{k=1}^K x_{u,i} \beta_k$ = The summation of all the genres' effects for that movie-user combination. 


Our goal is to find the $\beta$s to this equation in order to minimize the error. The error is the difference between the predicted value and the actual rating collected. To quantify this error we use the square root of the difference (or distance), which penalizes more the farther away the prediction is from the real data. Added a penalization term the equation for the _Root Mean Square Error_ is: 

$$
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$

where $y_{u,i}$ is the rating for movie $i$ by user $u$ and denote our prediction with $\hat{y}_{u,i}$. The penalty term $\lambda$ limits the total variability of the effect sizes, lest the model incorporates random variability unrelated to the theoretical variables we want to describe and generalize with our model. 

We now compare the application of this model with added genre weights and without it in the table below. 


```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}


# First, define the RMSE function:
RMSE <- function(true_ratings = NULL, predicted_ratings = NULL) {
  sqrt(mean((true_ratings - predicted_ratings)^2))
}




# MODEL 1: 
# Most basic assumption. Just go for the average of all movies. 
mu_hat <- mean(edx$rating)

# Predict the RMSE on the validation set
rmse_mean_model_result <- RMSE(validation$rating, mu_hat)

# Creating a results dataframe that contains all RMSE results
results <- data.frame(model="Mean-Baseline Model", RMSE=rmse_mean_model_result)



# Model 2: 
# Just the average, but with replication by genre. 
mu_hat_c <- mean(edx_copy$rating)

# Predict the RMSE on the validation set
rmse_mean_model_result_c <- RMSE(validation_copy$rating, mu_hat_c)

results <- results %>% add_row(model="Mean-Baseline Model with Weighed Genre", RMSE=rmse_mean_model_result_c)


# Model 3: User, movie and genre with regularization
# try new lambdas for the Regularized Movie+User+Genre Based Model. 

lambdas_mug <- seq(3, 7, 0.4)

# Compute the predicted ratings on validation dataset using different values of lambda

rmses <- sapply(lambdas_mug, function(lambda) {
  
  # Calculate the item effect
  b_i <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu_hat) / (n() + lambda))
  
  # Calculate user effect
  b_u <- edx %>%
    left_join(b_i, by='movieId') %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu_hat) / (n() + lambda))
  
  # Calculate the genre effect
  b_u_g <- edx %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    group_by(genres) %>%
    summarise(b_u_g = sum(rating - b_i - mu_hat - b_u) / (n() + lambda))
  
  # Compute the predicted ratings on validation dataset
  predicted_ratings <- validation %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_u_g, by='genres') %>%
    mutate(pred = mu_hat + b_i + b_u + b_u_g) %>%
    pull(pred)
  
  # Predict the RMSE on the validation set
  return(RMSE(validation$rating, predicted_ratings))
})

# check on the range of lambdas that produces the lowest RMSEs. It is about 15
df_mug <- data.frame(RMSE = rmses, lambdas = lambdas_mug)

# Get the lambda value that minimize the RMSE
min_lambda_mug <- lambdas_mug[which.min(rmses)]

# Predict the RMSE on the validation set
rmse_regularized_movie_user_genre_model <- min(rmses)

# Adding the results to the results dataset
results <- results %>% add_row(model="Regularized Movie+User+Genre Based Model", RMSE=rmse_regularized_movie_user_genre_model)



# Model 4: User, movie and genre with regularization and added weight to movies based on genre. 
# lambdas for the Regularized Movie+User+Genre Based Model. 

lambdas_mug_c <- seq(12, 18, 0.4)

# Compute the predicted ratings on validation dataset using different values of lambda

rmses_c <- sapply(lambdas_mug_c, function(lambda) {
  
  # Calculate the item effect
  b_i <- edx_copy %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu_hat_c) / (n() + lambda))
  
  # Calculate user effect
  b_u <- edx_copy %>%
    left_join(b_i, by='movieId') %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu_hat_c) / (n() + lambda))
  
  # Calculate the genre effect
  b_u_g <- edx_copy %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    group_by(genre) %>%
    summarise(b_u_g = sum(rating - b_i - mu_hat_c - b_u) / (n() + lambda))
  
  # Compute the predicted ratings on validation dataset
  predicted_ratings <- validation_copy %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_u_g, by='genre') %>%
    mutate(pred = mu_hat_c + b_i + b_u + b_u_g) %>%
    pull(pred)
  
  # Predict the RMSE on the validation set
  return(RMSE(validation_copy$rating, predicted_ratings))
})

# check on the range of lambdas that produces the lowest RMSEs. It is about 15
df_mug_c <- data.frame(RMSE = rmses_c, lambdas = lambdas_mug_c)

# Get the lambda value that minimize the RMSE
min_lambda_mug_c <- lambdas_mug_c[which.min(rmses_c)]

# Predict the RMSE on the validation set
rmse_regularized_movie_user_genre_model_c <- min(rmses_c)

# Adding the results to the results dataset
results <- results %>% add_row(model="Regularized Movie+User+Weighed Genre Based Model", RMSE=rmse_regularized_movie_user_genre_model_c)

```



```{r, echo=FALSE}
knitr::kable(
  results, align = 'lc'
)
```


```{r fig-reg1, fig.margin = TRUE, fig.show='hold', fig.cap = "Optimization of the Regularization parameter for the Linear models without added movie genre weights (top) and with it (bottom).", fig.width=3.5, fig.height=2.5, echo=FALSE, message=FALSE}


ggplot(df_mug, aes(lambdas_mug, rmses)) +
  theme_classic() +
  geom_point(color = "#00BEC4") +
  labs(title = "Reg. Mov+User+Gen Model",
       y = "RMSEs",
       x = "lambdas")



ggplot(df_mug_c, aes(lambdas_mug_c, rmses_c)) +
  theme_classic()  +
  geom_point(color = "#F8756D") +
  labs(title = "Reg. Mov+User+W.Gen Model",
       y = "RMSEs",
       x = "lambdas")

```

The difference in **RMSE** points to the predictive advantage in applying weights relative to the genres. 


# Data transformation for the Neural Network.  


For the second part of this analysis, we will use a _Deep Neural Network_. Several approaches are described in the literature and they are proved successful, of which one of the most popular might be the [winner of the Netflix challenge](https://www.researchgate.net/publication/228886759_The_BellKor_solution_to_the_Netflix_Grand_Prize) that used [Restricted Boltzmann Machines](https://www.sciencedirect.com/topics/computer-science/boltzmann-machine) as one of the algorithms. 

The output of the network could be a real number representing the rating of the user. But it is not possible, for example, a user to rate a movie 3.17 stars. That means the rating process can be best described as a classification task with multiple classes, and for that the _rating_ values must be transformed to factors or categories. 


## Genre Transformation

What is proposed here is that instead of feeding the neural network just the user and ratings, and expecting the appropriate relationships to be derived automatically, we first transform the data in order to give a multidimensional description of the user based on their affinity or preference for the genre. This is based on [personality psychology]() models that define traits as factors related to human behavior, which are derived from responses to questionnaires^[Popular personality theories based on this assumption include the Five Factor Model by [Costa and McCrae](https://www.researchgate.net/publication/284978581_A_five-factor_theory_of_personality), and the work of [Goldberg](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjZt4_zr9rrAhUAH7kGHeqPCC8QFjADegQIAxAB&url=https%3A%2F%2Fipip.ori.org%2FGoldberg_etal_2006_IPIP_JRP.pdf&usg=AOvVaw3nSzYVFMBZ0Ov3mDEN4WZu), and [DeYoung](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4818974/) to name a few.]. If a user can be described by the consistent of behaviors, then we can create a vector that describes a _userId_ using their average _rating_ for movies of a _genre_.  



## Timestamp Transformation

Because there is a difference in the number of values for ratings (classes) dependent on the time that rating was given, the predictions will be more accurate if the timestamp is informed. Nonetheless, there's no need to know the exact moment in time that rating was given. For that this variable will be transformed to be 0 or 1 assigned to a variable named _timestamp_binary_, which should be enough to determine if the rating is from a moment before or after the change in classes. 

The first columns of the final dataset are: 


```{r, echo=FALSE, cache=TRUE}


##########################################################
# Neural Network: Data transformation
##########################################################

# Mutate the timestamp to be 0 or 1 depending on the moment ratings start to have 0.5 granularity = 1045526400
edx <- edx %>% mutate(timestamp_binary = ifelse(edx$timestamp > 1045526400, 1, 0))
validation <- validation %>% mutate(timestamp_binary = ifelse(validation$timestamp > 1045526400, 1, 0))

############
# One-hot encoding of genres
############

genres <- as.data.frame(edx$genres, stringsAsFactors=FALSE)
genres_v <- as.data.frame(validation$genres, stringsAsFactors=FALSE)
# n_distinct(edx_copy$genres)
genres2 <- as.data.frame(tstrsplit(genres[,1], '[|]',
                                   type.convert=TRUE),
                         stringsAsFactors=FALSE)
genres2_v <- as.data.frame(tstrsplit(genres_v[,1], '[|]',
                                   type.convert=TRUE),
                         stringsAsFactors=FALSE)


genre_list <- c("Action", "Adventure", "Animation", "Children",
                "Comedy", "Crime","Documentary", "Drama", "Fantasy",
                "Film-Noir", "Horror", "Imax", "Musical", "Mystery","Romance",
                "Sci-Fi", "Thriller", "War", "Western") # There are 19 genres in total

genre_matrix <- matrix(0, length(edx$movieId)+1, n_distinct(genre_list))                       
genre_matrix[1,] <- genre_list #set first row to genre list

genre_matrix_v <- matrix(0, length(validation$movieId)+1, n_distinct(genre_list))                       
genre_matrix_v[1,] <- genre_list #set first row to genre list

colnames(genre_matrix) <- genre_list #set column names to genre list
colnames(genre_matrix_v) <- genre_list #set column names to genre list

#iterate through matrix
for (i in 1:nrow(genres2)) {
  for (c in 1:ncol(genres2)) {
    genmat_col <- which(genre_matrix[1,] == genres2[i,c])
    genre_matrix[i+1,genmat_col] <- 1L
  }
}

for (i in 1:nrow(genres2_v)) {
  for (c in 1:ncol(genres2_v)) {
    genmat_col <- which(genre_matrix_v[1,] == genres2_v[i,c])
    genre_matrix_v[i+1,genmat_col] <- 1L
  }
}
#convert into dataframe
genre_matrix <- as.data.frame(genre_matrix[-1,], stringsAsFactors=FALSE) #remove first row, which was the genre list
genre_matrix_v <- as.data.frame(genre_matrix_v[-1,], stringsAsFactors=FALSE)

edx_by_gen <- cbind(edx[,1:3], genre_matrix, edx$timestamp_binary) 
val_by_gen <- cbind(validation[,1:3], genre_matrix_v, validation$timestamp_binary)
colnames(edx_by_gen) <- c("userId", "movieId", "rating", genre_list, "timestamp_binary")
colnames(val_by_gen) <- c("userId", "movieId", "rating", genre_list, "timestamp_binary")
edx_by_gen <- as.matrix(sapply(edx_by_gen, as.numeric))
val_by_gen <- as.matrix(sapply(val_by_gen, as.numeric))


# remove intermediary matrices
# rm(genre_matrix, genre_matrix_v, genres, genres_v, genres2, genres2_v)


# Multiply the rating by the OHE for genre
edx_by_gen_mult <- cbind(edx_by_gen[,1:2], edx_by_gen[,"rating"], sweep(edx_by_gen[,4:22], 1, edx_by_gen[,"rating"], "*"), edx_by_gen[,"timestamp_binary"])
val_by_gen_mult <- cbind(val_by_gen[,1:2], val_by_gen[,"rating"], sweep(val_by_gen[,4:22], 1, val_by_gen[,"rating"], "*"), val_by_gen[,"timestamp_binary"])


colnames(edx_by_gen_mult) <- c("userId", "movieId", "rating", "Action", "Adventure", "Animation", "Children",
                          "Comedy", "Crime", "Documentary", "Drama", "Fantasy",
                          "Film.Noir", "Horror", "Imax", "Musical", "Mystery","Romance",
                          "Sci.Fi", "Thriller", "War", "Western", "timestamp_binary")

colnames(val_by_gen_mult) <- c("userId", "movieId", "rating", "Action", "Adventure", "Animation", "Children",
                               "Comedy", "Crime", "Documentary", "Drama", "Fantasy",
                               "Film.Noir", "Horror", "Imax", "Musical", "Mystery","Romance",
                               "Sci.Fi", "Thriller", "War", "Western", "timestamp_binary")


# Transform the multiplied one-hot-encoded matrix into a user profile for genre.
user_profiles <- edx_by_gen_mult %>%
  as.data.frame() %>%
  group_by(userId) %>%
  summarise(Action_u = mean(Action),
            Adventure_u = mean(Adventure),
            Animation_u = mean(Animation),
            Children_u = mean(Children),
            Comedy_u = mean(Comedy),
            Crime_u = mean(Crime),
            Documentary_u = mean(Documentary),
            Drama_u = mean(Drama),
            Fantasy_u = mean(Fantasy),
            FilmNoir_u = mean(Film.Noir),
            Horror_u = mean(Horror),
            Imax_u = mean(Imax), 
            Musical_u = mean(Musical),
            Mystery_u = mean(Mystery),
            Romance_u = mean(Romance),
            Sci.Fi_u = mean(Sci.Fi),
            Thriller_u = mean(Thriller),
            War_u = mean(War),
            Western_u = mean(Western)) %>%
  as.data.frame()


user_profiles[is.na(user_profiles)] <- 0

# Transform the Test and Validation datasets to include the user profiles
edx_gen_norm <- edx %>%
  left_join(user_profiles, by="userId") %>%
  select(userId, 
         movieId, 
         rating, 
         Action_u, 
         Adventure_u, 
         Animation_u,
         Children_u, 
         Comedy_u,  
         Crime_u,
         Documentary_u, 
         Drama_u,
         Fantasy_u,
         FilmNoir_u,  
         Horror_u, 
         Imax_u,
         Musical_u, 
         Mystery_u, 
         Romance_u, 
         Sci.Fi_u,  
         Thriller_u,  
         War_u, 
         Western_u, 
         timestamp_binary)

val_gen_norm <- validation %>%
  left_join(user_profiles, by="userId") %>%
  select(userId, 
         movieId, 
         rating, 
         Action_u, 
         Adventure_u, 
         Animation_u,
         Children_u, 
         Comedy_u,  
         Crime_u,
         Documentary_u, 
         Drama_u,
         Fantasy_u,
         FilmNoir_u,  
         Horror_u, 
         Imax_u,
         Musical_u, 
         Mystery_u, 
         Romance_u, 
         Sci.Fi_u,  
         Thriller_u,  
         War_u, 
         Western_u, 
         timestamp_binary)

```

```{r, echo=FALSE}
knitr::kable(
  head(edx_gen_norm[,1:6]), align = "c"
)
```


# Modeling approach 2 - Deep Neural Network

The neural network used here is a fully-connected neural network implemented by the [H2O package](https://www.h2o.ai/). The input data is a matrix of size _M_ (total number of ratings = `r nrow(edx_gen_norm)`) by _N_ (the columns: userId, movieId, 19 columns being one per genre, and the timestamp_binary that signals the change in output labels = `r ncol(edx_gen_norm)-1`). 

To calculate the conditional probabilities for each rating the _Softmax_ function will be used in the last layer of the network. The loss function used here is the automatically chosen based on the type of label on the validation data. Dropout (0.2) and early stopping were implemented to prevent overfitting. For more information, read the [documentation](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.html). 



```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Training will be commented for the generation of the report. Remove comments to train on your own machine. 

# #The following two commands remove any previously installed H2O packages for R.
# if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
# if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
# 
# # Next, we download packages that H2O depends on.
# pkgs <- c("RCurl","jsonlite")
# for (pkg in pkgs) {
#   if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
# }
# 
# # Now we download, install and initialize the H2O package for R.
# install.packages("h2o", type="source", repos="http://h2o-release.s3.amazonaws.com/h2o/rel-zeno/1/R")

# Finally, let's load H2O and start up an H2O cluster
# library(h2o)
h2o.init(nthreads = -1, max_mem_size = "8G")

##################
# Define the model in h2o

# turn the matrices into h2o objects
edx_h2o <- as.h2o(edx_gen_norm)
val_h2o <- as.h2o(val_gen_norm)

# Specify labels and predictors
y <- "rating"
x <- setdiff(names(edx_h2o), y)

# Turn the labels into categorical data.
edx_h2o[,y] <- as.factor(edx_h2o[,y])
val_h2o[,y] <- as.factor(val_h2o[,y])

# Train a deep learning model and validate on test set

DL_model <- h2o.deeplearning(
  x = x,
  y = y,
  training_frame = edx_h2o,
  validation_frame = val_h2o,
  distribution = "AUTO",
  activation = "RectifierWithDropout",
  hidden = c(256, 256, 256, 256),
  input_dropout_ratio = 0.2,
  sparse = TRUE,
  epochs = 15,
  stopping_rounds = 3,
  stopping_tolerance = 0.01, #stops if it doesn't improve at least 0.1%
  stopping_metric = "AUTO",
  nfolds = 10,
  variable_importances = TRUE,
  shuffle_training_data = TRUE,
  mini_batch_size = 2000
)



# Get RMSE
DL_RMSE_validation <- h2o.rmse(DL_model, valid = TRUE) # Validation RMSE = 0.8236556
DL_RMSE_training <- h2o.rmse(DL_model) # Train RMSE = 0.8241222

results <- results %>% add_row(model="Deep Neural Network", RMSE=DL_RMSE_validation)

```

```{r DNN_training, fig.margin = TRUE, fig.cap = "Classification error for training set and validation set  over epochs.", fig.width=3.5, fig.height=3.5, message=FALSE, echo=FALSE}

plot(DL_model)

```

# Evaluation of Models and Comparison of Results

The table below shows the performance difference regarding errors in the predictions for all models presented here. Though the training of the neural network was more than 10 times longer than the linear models the performance was also much better. This particular implementation had even a better performance than the one mentioned as the motivation for this project which had **RMSE** = **0.8572**. 

```{r, echo=FALSE}
knitr::kable(
  results, align = 'lc'
)
```


# Conclusion and Final Considerations

Deep Neural Networks are capable of modeling highly complex relationships between variables if properly structured. The risk of overfitting can be managed via the proper tuning of parameters and in the case of the implementation described here, the performance is better than the linear models that used the same variables. 

Knowledge of other areas, especially related to the problem at hand , is much valuable when creating hypothesis, defining models and transforming the data. If implementation expertise and final application of the solution can be combined the process of creation of models and interpretation of outputs will be more efficient. 




```{r bib, include=FALSE, echo=FALSE}
# create a bib file for the R packages used in this document
knitr::write_bib(c('base', 'rmarkdown'), file = 'skeleton.bib')
```
