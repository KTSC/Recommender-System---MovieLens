---
title: "Recommender System: Personality Theory Insights on the MovieLens Dataset" 
subtitle: "Harvard Data Science Professional Certificate Capstone Project"
author: "Klebert Toscano de S. Cintra"
date: FALSE
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
    toc: true
  tufte::tufte_html:
    toc: true
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE, echo=FALSE, warning=FALSE, message=FALSE}
library(tufte)
options(knitr.duplicate.label = "allow")
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
img_path <- paste(getwd(),"/img", sep = "")
```

\pagebreak

# Introduction

## Overview

Businesses in competitive markets are constantly seeking for tools that might be advantageous against the competitors. That is often related to the ability to anticipate preferences of their customers and offering products that the client will like, and not regret after. Also typical of competitive markets is the abundance of options to be chosen, and while this might seem like a desirable aspect of customization for the client, research[^1] has demonstrated that it makes the decision process more stressful, time consuming and, after the choice has been made, it is perceived as less satisfying. Providing good recommendations can attenuate the problems with cognitive overload, so there is high demand on the market to use data to guide the recommendations.

[^1]: This is a well documented psychological phenomenon. To know more read about the [jams experiment](https://psycnet.apa.org/record/2000-16701-012) and this [meta-analysis on choice overload](https://archive-ouverte.unige.ch/unige:76440). Barry Schwartz's book [The Paradox of Choice](https://www.amazon.com/Paradox-Choice-Why-More-Less/dp/149151423X) also covers the subject nicely.

Here we try two different approaches to make recommendations using data of a very competitive market: the movie industry.

## Executive Summary

Among the many methods for recommender systems a very popular one due to its simplicity, interpretability and having low computational demands is the search for a function that best describes the relationship between two or more variables and by doing so, make predictions. This is called a **Linear Model**. The weights of the variables to be combined are then estimated by minimizing the distance between the observed data and the line generated by the function. This is the Least Squares method and the estimates for the weights for the variables are the **Least Squares Estimates (LSE)**.

```{marginfigure}
The **Least Squares Estimates** method describes how a random variable we want to predict $Y$ is defined by the linear combination of the independent variables $x_i$ given the weights $\beta$ and some random error $\varepsilon$:
$$Y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_i x_i + \varepsilon_i, \, i=1,\dots,N.$$
```
  
  
The other method applied here makes use of **Deep Neural Networks**, which are very popular due to their applicability to many types of data. Unlike **LSE**, it can be quite computationally demanding, and not easily interpretable. They are often represented using units (neurons) that are aggregating functions, organized in layers that are able to assign numeric value to different levels of abstraction on the input data. The number of layers constitutes the *depth* of the neural network.

```{r, echo = FALSE, out.width = "140%", warning=FALSE, message=FALSE}
knitr::include_graphics(file.path(img_path, "nn.png"))
```

`r newthought('The goal')` of this project is to compare these two approaches in a large dataset to create a recommender system. The data used here is the [*MovieLens Dataset*](https://grouplens.org/datasets/movielens/latest/) consisting of 10 million ratings. The recommender system implemented here is inspired by the famous [Netflix Prize](https://www.netflixprize.com/index.html), which awarded the [winners](http://www.research.att.com/~volinsky/netflix/bpc.html) 1 Million Dollars for getting the **Root Mean Squared Error (RMSE)** of **0.8572**. We will compare different methods using this metric.  


```{r Create test and validation sets, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)


# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
# movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                            title = as.character(title),
#                                            genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
# set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

```{r Loading packages, message=FALSE, warning=FALSE, echo=FALSE, results="hide", cache=TRUE}
library(ggplot2)
library(lubridate)

if(!require(extrafont)) install.packages("extrafont")
library(extrafont)

font_import()
loadfonts(device = "pdf", quiet = TRUE)
```

`r newthought('The 10M MovieLens dataset')` has ratings by `r length(unique(edx$userId))` unique users for `r length(unique(edx$movieId))` movies. The ratings consist in assigning a number of stars compatible with the appreciation of the user for a particular movie. The first lines of the data frame can be seen below with a short description of variables to the right.  


```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
knitr::kable(
  head(edx, 5), align = 'c', 
  caption = "The MovieLens Data Set"
)
```

`r margin_note("VARIABLES")`  

`r margin_note("userId: unique identifier for user.")` 
`r margin_note("movieId: unique identifier for movie.")` 
`r margin_note("rating: how a user rated a particular movie.")` 
`r margin_note("timestamp: the time when the rating happened in seconds since 01/01/1970.")` 
`r margin_note("title: the title of the movie.")` 
`r margin_note("genres: the genres that describe the movie.")`

The procedure proposed here includes the following:

1.  Data acquisition and cleaning. Download and partition of the data with 10% of observations for validation and 90% for training of the algorithm.
2.  Exploratory Data Analysis (EDA) and Visualization.\
3.  Data transformation for the Linear Models.
4.  Modeling approach 1 - Linear Models.
5.  Data transformation for the Deep Neural Network.
6.  Modeling approach 2 - Deep Neural Network.
7.  Evaluation of models and comparison of results.
8.  Conclusion and final considerations.

```{r, echo = FALSE, out.width = "130%", fig.cap = "Infographic of the framework adopted for this recommender system. Depicts the cyclical nature of the model development that ultimately might go to deployment/production.", warning=FALSE, message=FALSE, cache=TRUE}
knitr::include_graphics(file.path(img_path,"Data_Science_cycle.png"))
```

# Exploratory Data Analysis (EDA) and Visualization

This report will focus on the relevant visualizations for the methods of choice, and is not an exhaustive assessment of the data.

We start with the value we want to predict, which is the *rating* that a user would give to a movie. The ratings range from 0.5 stars to 5 stars in steps of 0.5, resulting in 10 different possible ratings.

```{r fig-main, echo = FALSE, fig.width=7, fig.height=4, message=FALSE, warning=FALSE, cache=TRUE}

ggplot(data = edx, aes(x = rating)) + 
  geom_bar() + 
  labs(title = "Distribution of Ratings in the Training Set", x = "Rating", y = "Frequency") +
  theme_minimal(base_size = 12, base_family = "Caviar Dreams")

```


```{r fig-margin-splittime, echo = FALSE, fig.margin=TRUE, fig.show='hold', fig.cap="Upper plot shows the ratings distribution before timestamp 1045526400 (corresponding to the date 02/18/2003), with granularity of 1. The lower panel shows the distribution after this timestamp, with granularity of 0.5.", fig.width=3.5, fig.height=2.5, warning=FALSE, message=FALSE, cache=TRUE}

edx_previous <- subset(edx, timestamp < 1045526400)
edx_after <- subset(edx, timestamp >= 1045526400)

# Ratings before 1045526400
plot_previous <- ggplot(data = edx_previous, aes(x = rating)) +
  geom_bar(fill = "#00BEC4") + 
  labs(title = "Until Feb 18, 2003", x = "Rating", y = "Frequency") +
  theme_minimal(base_size = 11, base_family = "Caviar Dreams")
plot_previous
#...and after 1045526400
plot_after <- ggplot(data = edx_after, aes(x = rating)) +
  geom_bar(fill = "#F8756D") + 
  labs(title = "After Feb 18, 2003", x = "Rating", y = "Frequency") +
  theme_minimal(base_size = 11, base_family = "Caviar Dreams")
plot_after
```

The *timestamp* is the count of seconds since January 01 of 1970. Visual inspection of the dataset shows a change in the way the ratings are distributed. Until February 18 of 2003 (timestamp = 1045526400) the ratings are only integer numbers. After that moment, instead of 5 possible ratings (1 to 5) there are 10 different possibilities.  

If we repeat the plot of frequency for ratings with the bars colored based on this particular timestamp we have clear indication this is a relevant aspect to the value we want to predict, and for that reason, this should be taken into consideration.  
  
  


```{r, echo=FALSE, fig.width=7, fig.height=4, fig.cap= "Distribution of the ratings for all users in the edx partition, before and after February 18 of 2003. (Figure to the left)", message=FALSE, warning=FALSE, fig.align='left', cache=TRUE}

ggplot(data = edx, aes(x = rating, fill = timestamp > 1045526400)) + 
  geom_bar() + 
  scale_fill_manual(values=c("#00BEC4", "#F8756D")) + 
  labs(title = "Distribution of Ratings split by timestamp", x = "Rating", y = "Frequency") +
  theme(legend.position="bottom") +
  theme_minimal(base_size = 12, base_family = "Caviar Dreams")
  

```
  
    
    
In addition to this labeling difference dependent on time, we can look for other time-dependent effects. See the picture below and observe the average ratings by month seem to vary slightly. Observe also the clear anomaly in ratings for the early years, showing much higher rating average by month than the remaining time window.

```{r ratings-time-series, echo = FALSE, fig.cap= "A time series of the average of ratings for all users, one point per month.", cache=TRUE, message=FALSE, warning=FALSE}

edx_copy <- edx
edx_copy %>% 
  mutate(Month = round_date(as_datetime(timestamp), unit = "month")) %>%
  group_by(Month) %>%
  summarize(Rating = mean(rating)) %>%
  ggplot(aes(Month, Rating)) +
  geom_point() +
  geom_smooth(color="#F8756D") +
  ggtitle("Time series of average rating") +
  labs(subtitle = "Training data only", x = "Time", y = "Average Rating") +
  theme_minimal(base_size = 12, base_family = "Caviar Dreams")
```

As previously shown in *table 1*, numeric values are used as unique identifiers for users and movies, but movies also have a variable for the movie title, which include the year of release between parenthesis. This can be used in future analysis, but for the scope of this project we will not consider characteristics of the movie title or the year it was released.

The key variable to be examined is the *genres* column of the dataset. It is a way of describing movies implying a relationship in a higher level of abstraction between movies. It consists of strings with all the genres in which a movie can be classified separated by the character "\|". Here we can see the boxplots for the aggregated ratings on all the listed genres.

```{r genres-boxplots, fig.fullwidth=TRUE, echo = FALSE, fig.cap= "Boxplots of ratings grouped by genre.", message=FALSE, warning=FALSE, cache=TRUE}

library(tidyverse)

edx %>% 
  separate_rows(genres, sep = "\\|") %>% 
  select(genres, rating) %>% 
  group_by(genres) %>%
  ggplot(aes(x = factor(genres), y = rating, fill = genres)) +
  geom_boxplot(show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, family = "Caviar Dreams")) +
  xlab("Movie Genre") +
  ylab("Rating")

```

There's no obvious difference between genres to make a prediction based on this dimension alone.

To finish this exploratory data analysis we check on the distribution of user rating averages and the average for movies.

```{r user_and_movie_histograms, echo = FALSE, fig.cap="Histograms of average ratings for user (top) and for movies (bottom).", fig.show='hold', message=FALSE, warning=FALSE, cache=TRUE}

edx %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating)) %>%
  filter(n()>= 40) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color =I("#00BEC4")) +
  ggtitle("Histogram of User's average ratings") +
  labs(subtitle = "Average rating") +
  theme_minimal(base_size = 12, base_family = "Caviar Dreams")

edx %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating)) %>%
  filter(n()>= 40) %>%
  ggplot(aes(b_i)) + 
  geom_histogram(bins = 30, color =I("#F8756D")) +
  ggtitle("Histogram of Movie's average ratings") +
  labs(subtitle = "Average rating") +
  theme_minimal(base_size = 12, base_family = "Caviar Dreams")
```

\pagebreak

# Data Transformation for the Linear Model

The transformations applied to data are based in the following assumptions:

1.  Movie genres provide valuable information about abstract dimensions of the movies. A genre usually have a typical pace of the story, the content of the scenes, saturation of colors, angles for the cameras, emotions provoked, the of casting of actors, and all of those influence the expectations of the audience and might interfere with the ratings even before a movie is watched. So, it could be a less computationally expensive way to numerically describe a movie[^2].

2.  The evaluation a user provides to a single movie has little generalization for other movies, but the consistent rating of a user for a genre (be it positive or negative) describes the subject[^3].

3.  The more genres a movie has, the more eclectic it might be. Hence, it might attract a larger audience, which result in more ratings, and as a consequence, greater predictive value.

4.  The label to be predicted is not a continuous value ranging from 0 to 5. Instead, they are categories that depend on the timestamp variable, as demonstrated in the pictures above.

[^2]: Matrix Factorization using **Principal Component Analysis** and other transformations on this dataset made necessary more than 2000 *principal components* to explain 90% of the variability on data. Hence, for each prediction on the pairing User-Movie a 2000x2000 vector should be multiplied, and the estimation for 1 million data points would be costly with common personal computers' CPUs.

[^3]: If an individual shows a consistent preference or response that is stable over time and different contexts is considered a **trait**. Trait psychology and the study of individual differences can be applied to the creation or refinement of recommender systems. The book [Hierarchical Cognitive Models](https://www.cambridge.org/us/academic/subjects/psychology/psychology-research-methods-and-statistics/bayesian-cognitive-modeling-practical-course) has many examples of applications for inferences on latent traits.

## Genre Transformation for the Linear Model

The proposed procedure is to replicate each line for a "userId" + "movieId" pairing for each unique genre this movie has as attribute.

Let's use as example the first line of the original dataset. We can see there is one pairing of userId = 1 and movieId = 122:

```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
knitr::kable(
  head(edx, 5), align = 'c'
)
```

After the transformation, this pair (userId = 1, movieId = 122) will be featured twice, for having 2 genres for this movie: "Comedy" and "Romance".

```{r, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}

##########################################################
# Make a copy of the original train and test datasets.  
##########################################################

edx_copy <- edx
validation_copy <- validation

##########################################################
# Separate the genres. These lines replicates each ratings for every genre on the movie. 
##########################################################

edx_copy$genres <- lapply(edx_copy$genres, as.character)
validation_copy$genres <- lapply(validation_copy$genres, as.character)
edx_copy <- edx_copy %>% 
  mutate(genre = as.character(genres)) %>%
  separate_rows(genre, sep = "\\|") %>%
  select(-"genres", -"timestamp", -"title")

# Extract the genre in validation datasets
validation_copy <- validation_copy %>% 
  mutate(genre = as.character(genres)) %>%
  separate_rows(genre, sep = "\\|")%>%
  select(-"genres", -"timestamp", -"title")


knitr::kable(
  head(edx_copy, 5), align = 'c'
)
```

The variables *timestamp* and *title* were removed as they are not used for this analysis.

# Modeling approach 1 - Linear Models.

This class of models is based on the premise that the response variable $Y_{u,i}$ representing the rating user $u$ assigned to movie $i$ is equal to a theoretical film rating described by the linear combination of some random variables. The model definition is:

$Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}$, with $x^k_{u,i} = 1$ if $g_{u,i}$ is genre $k$.

where:

$\mu$ = The mean of all ratings.

$b_u$ = The user effect.

$b_i$ = The item effect, in this case, the movie.

$g_{u,i}$ = Genre for user $u$ rating of movie $i$.

$\varepsilon_{u,i}$ = The error. Randomness on the data, noise in the system.

$\sum_{k=1}^K x_{u,i} \beta_k$ = The summation of all the genres' effects for that movie-user combination.

Our goal is to find the $\beta$s to this equation in order to minimize the error. The error is the difference between the predicted value and the actual rating collected. To quantify this error we use the square root of the difference (or distance), which penalizes more the farther away the prediction is from the real data. Added a penalization term the equation for the *Root Mean Square Error* is:

$$
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$

where $y_{u,i}$ is the rating for movie $i$ by user $u$ and denote our prediction with $\hat{y}_{u,i}$. The penalty term $\lambda$ limits the total variability of the effect sizes, lest the model incorporates random variability unrelated to the theoretical variables we want to describe and generalize with our model.

We now compare the application of this model with added genre weights and without it in the table below.

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}


# First, define the RMSE function:
RMSE <- function(true_ratings = NULL, predicted_ratings = NULL) {
  sqrt(mean((true_ratings - predicted_ratings)^2))
}




# MODEL 1: 
# Most basic assumption. Just go for the average of all movies. 
mu_hat <- mean(edx$rating)

# Predict the RMSE on the validation set
rmse_mean_model_result <- RMSE(validation$rating, mu_hat)

# Creating a results dataframe that contains all RMSE results
results <- data.frame(model="Mean-Baseline Model", RMSE=rmse_mean_model_result)



# Model 2: 
# Just the average, but with replication by genre. 
mu_hat_c <- mean(edx_copy$rating)

# Predict the RMSE on the validation set
rmse_mean_model_result_c <- RMSE(validation_copy$rating, mu_hat_c)

results <- results %>% add_row(model="Mean-Baseline Model with Weighed Genre", RMSE=rmse_mean_model_result_c)


# Model 3: User, movie and genre with regularization
# try new lambdas for the Regularized Movie+User+Genre Based Model. 

lambdas_mug <- seq(3, 7, 0.4)

# Compute the predicted ratings on validation dataset using different values of lambda

rmses <- sapply(lambdas_mug, function(lambda) {
  
  # Calculate the item effect
  b_i <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu_hat) / (n() + lambda))
  
  # Calculate user effect
  b_u <- edx %>%
    left_join(b_i, by='movieId') %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu_hat) / (n() + lambda))
  
  # Calculate the genre effect
  b_u_g <- edx %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    group_by(genres) %>%
    summarise(b_u_g = sum(rating - b_i - mu_hat - b_u) / (n() + lambda))
  
  # Compute the predicted ratings on validation dataset
  predicted_ratings <- validation %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_u_g, by='genres') %>%
    mutate(pred = mu_hat + b_i + b_u + b_u_g) %>%
    pull(pred)
  
  # Predict the RMSE on the validation set
  return(RMSE(validation$rating, predicted_ratings))
})

# check on the range of lambdas that produces the lowest RMSEs. It is about 15
df_mug <- data.frame(RMSE = rmses, lambdas = lambdas_mug)

# Get the lambda value that minimize the RMSE
min_lambda_mug <- lambdas_mug[which.min(rmses)]

# Predict the RMSE on the validation set
rmse_regularized_movie_user_genre_model <- min(rmses)

# Adding the results to the results dataset
results <- results %>% add_row(model="Regularized Movie+User+Genre Based Model", RMSE=rmse_regularized_movie_user_genre_model)



# Model 4: User, movie and genre with regularization and added weight to movies based on genre. 
# lambdas for the Regularized Movie+User+Genre Based Model. 

lambdas_mug_c <- seq(12, 18, 0.4)

# Compute the predicted ratings on validation dataset using different values of lambda

rmses_c <- sapply(lambdas_mug_c, function(lambda) {
  
  # Calculate the item effect
  b_i <- edx_copy %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu_hat_c) / (n() + lambda))
  
  # Calculate user effect
  b_u <- edx_copy %>%
    left_join(b_i, by='movieId') %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu_hat_c) / (n() + lambda))
  
  # Calculate the genre effect
  b_u_g <- edx_copy %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    group_by(genre) %>%
    summarise(b_u_g = sum(rating - b_i - mu_hat_c - b_u) / (n() + lambda))
  
  # Compute the predicted ratings on validation dataset
  predicted_ratings <- validation_copy %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_u_g, by='genre') %>%
    mutate(pred = mu_hat_c + b_i + b_u + b_u_g) %>%
    pull(pred)
  
  # Predict the RMSE on the validation set
  return(RMSE(validation_copy$rating, predicted_ratings))
})

# check on the range of lambdas that produces the lowest RMSEs. It is about 15
df_mug_c <- data.frame(RMSE = rmses_c, lambdas = lambdas_mug_c)

# Get the lambda value that minimize the RMSE
min_lambda_mug_c <- lambdas_mug_c[which.min(rmses_c)]

# Predict the RMSE on the validation set
rmse_regularized_movie_user_genre_model_c <- min(rmses_c)

# Adding the results to the results dataset
results <- results %>% add_row(model="Regularized Movie+User+Weighed Genre Based Model", RMSE=rmse_regularized_movie_user_genre_model_c)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
knitr::kable(
  results, align = 'lc'
)
```

```{r fig-reg1, fig.margin = TRUE, fig.show='hold', fig.cap = "Optimization of the Regularization parameter for the Linear models without added movie genre weights (top) and with it (bottom).", fig.width=3.5, fig.height=2.5, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}


ggplot(df_mug, aes(lambdas_mug, rmses)) +
  theme_classic() +
  geom_point(color = "#00BEC4") +
  labs(title = "Reg. Mov+User+Gen Model",
       y = "RMSEs",
       x = "lambdas") +
  theme_minimal(base_size = 11, base_family = "Caviar Dreams")



ggplot(df_mug_c, aes(lambdas_mug_c, rmses_c)) +
  theme_classic()  +
  geom_point(color = "#F8756D") +
  labs(title = "Reg. Mov+User+W.Gen Model",
       y = "RMSEs",
       x = "lambdas") +
  theme_minimal(base_size = 11, base_family = "Caviar Dreams")

```

The difference in **RMSE** points to the predictive advantage in applying weights relative to the genres.

\pagebreak

# Data transformation for the Neural Network.

For the second part of this analysis, we will use a *Deep Neural Network*. Several approaches are described in the literature and they are proved successful, of which one of the most popular might be the [winner of the Netflix challenge](https://www.researchgate.net/publication/228886759_The_BellKor_solution_to_the_Netflix_Grand_Prize) that used [Restricted Boltzmann Machines](https://www.sciencedirect.com/topics/computer-science/boltzmann-machine) as one of the algorithms.

The output of the network could be a real number representing the rating of the user. But it is not possible, for example, a user to rate a movie 3.17 stars. That means the rating process can be best described as a classification task with multiple classes, and for that the *rating* values must be transformed to factors or categories.

## Genre Transformation

What is proposed here is that instead of feeding the neural network just the user and ratings, and expecting the appropriate relationships to be derived automatically, we first transform the data in order to give a multidimensional description of the user based on their affinity or preference for the genre. This is based on [personality psychology]() models that define traits as factors related to human behavior, which are derived from responses to questionnaires[^4]. If a user can be described by the consistent of behaviors, then we can create a vector that describes a *userId* using their average *rating* for movies of a *genre*.

[^4]: Popular personality theories based on this assumption include the Five Factor Model by [Costa and McCrae](https://www.researchgate.net/publication/284978581_A_five-factor_theory_of_personality), and the work of [Goldberg](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjZt4_zr9rrAhUAH7kGHeqPCC8QFjADegQIAxAB&url=https%3A%2F%2Fipip.ori.org%2FGoldberg_etal_2006_IPIP_JRP.pdf&usg=AOvVaw3nSzYVFMBZ0Ov3mDEN4WZu), and [DeYoung](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4818974/) to name a few.

## Timestamp Transformation

Because there is a difference in the number of values for ratings (classes) dependent on the time that rating was given, the predictions will be more accurate if the timestamp is informed. Nonetheless, there's no need to know the exact moment in time that rating was given. For that this variable will be transformed to be 0 or 1 assigned to a variable named *timestamp_binary*, which should be enough to determine if the rating is from a moment before or after the change in classes.

The first columns of the final dataset are:

```{r, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}


##########################################################
# Neural Network: Data transformation
##########################################################

# Mutate the timestamp to be 0 or 1 depending on the moment ratings start to have 0.5 granularity = 1045526400
edx <- edx %>% mutate(timestamp_binary = ifelse(edx$timestamp > 1045526400, 1, 0))
validation <- validation %>% mutate(timestamp_binary = ifelse(validation$timestamp > 1045526400, 1, 0))

############
# One-hot encoding of genres
############

genres <- as.data.frame(edx$genres, stringsAsFactors=FALSE)
genres_v <- as.data.frame(validation$genres, stringsAsFactors=FALSE)
# n_distinct(edx_copy$genres)
genres2 <- as.data.frame(tstrsplit(genres[,1], '[|]',
                                   type.convert=TRUE),
                         stringsAsFactors=FALSE)
genres2_v <- as.data.frame(tstrsplit(genres_v[,1], '[|]',
                                   type.convert=TRUE),
                         stringsAsFactors=FALSE)


genre_list <- c("Action", "Adventure", "Animation", "Children",
                "Comedy", "Crime","Documentary", "Drama", "Fantasy",
                "Film-Noir", "Horror", "Imax", "Musical", "Mystery","Romance",
                "Sci-Fi", "Thriller", "War", "Western") # There are 19 genres in total

genre_matrix <- matrix(0, length(edx$movieId)+1, n_distinct(genre_list))                       
genre_matrix[1,] <- genre_list #set first row to genre list

genre_matrix_v <- matrix(0, length(validation$movieId)+1, n_distinct(genre_list))                       
genre_matrix_v[1,] <- genre_list #set first row to genre list

colnames(genre_matrix) <- genre_list #set column names to genre list
colnames(genre_matrix_v) <- genre_list #set column names to genre list

#iterate through matrix
for (i in 1:nrow(genres2)) {
  for (c in 1:ncol(genres2)) {
    genmat_col <- which(genre_matrix[1,] == genres2[i,c])
    genre_matrix[i+1,genmat_col] <- 1L
  }
}

for (i in 1:nrow(genres2_v)) {
  for (c in 1:ncol(genres2_v)) {
    genmat_col <- which(genre_matrix_v[1,] == genres2_v[i,c])
    genre_matrix_v[i+1,genmat_col] <- 1L
  }
}
#convert into dataframe
genre_matrix <- as.data.frame(genre_matrix[-1,], stringsAsFactors=FALSE) #remove first row, which was the genre list
genre_matrix_v <- as.data.frame(genre_matrix_v[-1,], stringsAsFactors=FALSE)

edx_by_gen <- cbind(edx[,1:3], genre_matrix, edx$timestamp_binary) 
val_by_gen <- cbind(validation[,1:3], genre_matrix_v, validation$timestamp_binary)
colnames(edx_by_gen) <- c("userId", "movieId", "rating", genre_list, "timestamp_binary")
colnames(val_by_gen) <- c("userId", "movieId", "rating", genre_list, "timestamp_binary")
edx_by_gen <- as.matrix(sapply(edx_by_gen, as.numeric))
val_by_gen <- as.matrix(sapply(val_by_gen, as.numeric))


# remove intermediary matrices
# rm(genre_matrix, genre_matrix_v, genres, genres_v, genres2, genres2_v)


# Multiply the rating by the OHE for genre
edx_by_gen_mult <- cbind(edx_by_gen[,1:2], edx_by_gen[,"rating"], sweep(edx_by_gen[,4:22], 1, edx_by_gen[,"rating"], "*"), edx_by_gen[,"timestamp_binary"])
val_by_gen_mult <- cbind(val_by_gen[,1:2], val_by_gen[,"rating"], sweep(val_by_gen[,4:22], 1, val_by_gen[,"rating"], "*"), val_by_gen[,"timestamp_binary"])


colnames(edx_by_gen_mult) <- c("userId", "movieId", "rating", "Action", "Adventure", "Animation", "Children",
                          "Comedy", "Crime", "Documentary", "Drama", "Fantasy",
                          "Film.Noir", "Horror", "Imax", "Musical", "Mystery","Romance",
                          "Sci.Fi", "Thriller", "War", "Western", "timestamp_binary")

colnames(val_by_gen_mult) <- c("userId", "movieId", "rating", "Action", "Adventure", "Animation", "Children",
                               "Comedy", "Crime", "Documentary", "Drama", "Fantasy",
                               "Film.Noir", "Horror", "Imax", "Musical", "Mystery","Romance",
                               "Sci.Fi", "Thriller", "War", "Western", "timestamp_binary")


# Transform the multiplied one-hot-encoded matrix into a user profile for genre.
user_profiles <- edx_by_gen_mult %>%
  as.data.frame() %>%
  group_by(userId) %>%
  summarise(Action_u = mean(Action),
            Adventure_u = mean(Adventure),
            Animation_u = mean(Animation),
            Children_u = mean(Children),
            Comedy_u = mean(Comedy),
            Crime_u = mean(Crime),
            Documentary_u = mean(Documentary),
            Drama_u = mean(Drama),
            Fantasy_u = mean(Fantasy),
            FilmNoir_u = mean(Film.Noir),
            Horror_u = mean(Horror),
            Imax_u = mean(Imax), 
            Musical_u = mean(Musical),
            Mystery_u = mean(Mystery),
            Romance_u = mean(Romance),
            Sci.Fi_u = mean(Sci.Fi),
            Thriller_u = mean(Thriller),
            War_u = mean(War),
            Western_u = mean(Western)) %>%
  as.data.frame()


user_profiles[is.na(user_profiles)] <- 0

# Transform the Test and Validation datasets to include the user profiles
edx_gen_norm <- edx %>%
  left_join(user_profiles, by="userId") %>%
  select(userId, 
         movieId, 
         rating, 
         Action_u, 
         Adventure_u, 
         Animation_u,
         Children_u, 
         Comedy_u,  
         Crime_u,
         Documentary_u, 
         Drama_u,
         Fantasy_u,
         FilmNoir_u,  
         Horror_u, 
         Imax_u,
         Musical_u, 
         Mystery_u, 
         Romance_u, 
         Sci.Fi_u,  
         Thriller_u,  
         War_u, 
         Western_u, 
         timestamp_binary)

val_gen_norm <- validation %>%
  left_join(user_profiles, by="userId") %>%
  select(userId, 
         movieId, 
         rating, 
         Action_u, 
         Adventure_u, 
         Animation_u,
         Children_u, 
         Comedy_u,  
         Crime_u,
         Documentary_u, 
         Drama_u,
         Fantasy_u,
         FilmNoir_u,  
         Horror_u, 
         Imax_u,
         Musical_u, 
         Mystery_u, 
         Romance_u, 
         Sci.Fi_u,  
         Thriller_u,  
         War_u, 
         Western_u, 
         timestamp_binary)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
knitr::kable(
  head(edx_gen_norm[,1:6], 5), align = "c"
)
```

# Modeling approach 2 - Deep Neural Network

A Neural Network is a system where the inputs are automatically transformed in a way to learn output patterns. The metaphor with the biological neural networks found in the nervous system of animals relies on the neuron, that in biological nervous systems is the functional unit. In an artificial neural network it is an abstraction consisting of a function that has the following stages: 1) takes in values as inputs in a multidimensional data structure called a *tensor*; 2) aggregates the inputs into a unique value'; 3) assigns to each one of the input sources a weight that represents its relevance to make a prediction; 4) the aggregated value is passed on to an activation function, which defines the final output for that particular neuron. The output is compared to a known correct value of the dimension the system is to predict, and the errors in the prediction are used to correct the weights of that neuron.

```{r, echo = FALSE, out.width = "140%", fig.cap = "Illustration of an Artificial Neuron with representation of inputs, aggregation of inputs and production of an output by the activation function. ", warning=FALSE, message=FALSE, cache=TRUE}
knitr::include_graphics(file.path(img_path,"NN_units_basic.png"))
```

Neurons can be organized in a great variety of ways, and the process of adjusting all the parameters of the chain of functions is called *training*, borrowing a nomenclature from used in cognitive psychology and learning neuroscience. The neural network used here is a fully-connected neural network with 4 layers of 256 neurons each, where each neuron of one layer connects to all neurons in the next layer. This architecture is implemented by the [H2O package](https://www.h2o.ai/). The input data is a matrix of size *M* (total number of ratings = `r nrow(edx_gen_norm)`) by *N* (the columns: userId, movieId, 19 columns being one per genre, and the timestamp_binary that signals the change in output labels = `r ncol(edx_gen_norm)-1`).

```{r, echo = FALSE, out.width = "130%", fig.cap = "Architecture of the Neural Network used here.", warning=FALSE, message=FALSE, cache=TRUE}
knitr::include_graphics(file.path(img_path,"NN_architecture.png"))
```

\pagebreak

```{r, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, results="hide"}

# Training will be commented for the generation of the report. Remove comments to train on your own machine.

## The following two commands remove any previously installed H2O packages for R.
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }

## Next, we download packages that H2O depends on.
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}

### Now we download, install and initialize the H2O package for R.
install.packages("h2o", type="source", repos="http://h2o-release.s3.amazonaws.com/h2o/rel-zeno/1/R")

## Finally, let's load H2O and start up an H2O cluster
library(h2o)
h2o.init(nthreads = -1, max_mem_size = "12G")

##################
# Define the model in h2o

# turn the matrices into h2o objects
edx_h2o <- as.h2o(edx_gen_norm)
val_h2o <- as.h2o(val_gen_norm)

# Specify labels and predictors
y <- "rating"
x <- setdiff(names(edx_h2o), y)

# Turn the labels into categorical data.
edx_h2o[,y] <- as.factor(edx_h2o[,y])
val_h2o[,y] <- as.factor(val_h2o[,y])

# Train a deep learning model and validate on test set

DL_model <- h2o.deeplearning(
  x = x,
  y = y,
  training_frame = edx_h2o,
  validation_frame = val_h2o,
  distribution = "AUTO",
  activation = "RectifierWithDropout",
  hidden = c(256, 256, 256, 256),
  input_dropout_ratio = 0.15,
  sparse = TRUE,
  epochs = 15,
  stopping_rounds = 5,
  stopping_tolerance = 0.01, #stops if it doesn't improve at least 1%
  stopping_metric = "AUTO", # Because it is a classification, the metric is classification_error
  # stopping_metric = "RMSE",
  nfolds = 5,
  variable_importances = TRUE,
  shuffle_training_data = TRUE,
  mini_batch_size = 4000,
  overwrite_with_best_model = TRUE,
  quiet_mode = TRUE,
  l1=1e-5,                        ## add some L1/L2 regularization
  l2=1e-5,
  max_w2=10                       ## helps stability for Rectifier
)


##### Change after training.
# Save the model
DL_model_path <- h2o.saveModel(object = DL_model, path = getwd(), force =TRUE)




# load the model
DL_model <- h2o.loadModel(DL_model_path)

# Get RMSE
DL_RMSE_validation <- h2o.rmse(DL_model, valid = TRUE)
DL_RMSE_training <- h2o.rmse(DL_model)

results <- results %>% add_row(model="Deep Neural Network", RMSE=DL_RMSE_validation)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results="hide"} 
library(tidyverse)
library(h2o)
h2o.init(nthreads = -1, max_mem_size = "12G")

# Get RMSE
DL_RMSE_validation <- h2o.rmse(DL_model, valid = TRUE)
DL_RMSE_training <- h2o.rmse(DL_model)

results <- results %>% add_row(model="Deep Neural Network", RMSE=DL_RMSE_validation)

```

To calculate the conditional probabilities for each rating the *Softmax* function will be used in the last layer of the network. The loss function used here is automatically chosen based on the type of label on the validation data. Dropout (0.2) and early stopping are implemented to prevent overfitting. For more information, read the [documentation](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.html).

```{r DNN_training, fig.margin = TRUE, fig.cap = "Classification error for training set and validation set  over epochs.", fig.width=3.5, fig.height=3.5, message=FALSE, echo=FALSE}

plot(DL_model)

```

# Evaluation of Models and Comparison of Results

The table below shows the performance difference regarding errors in the predictions for all models presented here. Though the training of the neural network took several times longer than the linear models using CPUs, the performance was also much better. This particular implementation had **RMSE** = `r round(DL_RMSE_validation, 4)`, a better performance than the one mentioned as the motivation for this project which had **RMSE** = **0.8572**.

```{r, echo = FALSE, fig.width = 8, fig.height = 4, fig.cap = "Barplot of the error rate of the models used.", warning=FALSE, message=FALSE}

ggplot(data = results, aes(x = model, y = RMSE, label = RMSE, fill = RMSE)) + 
  geom_bar(stat = "identity", width = 0.4) + 
  scale_fill_viridis_c() + 
  ggtitle("Error Rate Comparison") +
  xlab("Models") +
  ylab("RMSE") + 
  theme_minimal(base_size = 12, base_family = "Caviar Dreams") +
  geom_text(aes(label=round(RMSE, 4)), hjust=1.2, color="white", size=2.5) +
  coord_flip()


```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::kable(
  results, align = 'lc'
)
```

# Conclusion and Final Considerations

Deep Neural Networks are capable of modeling highly complex relationships between variables if properly structured. The risk of overfitting can be managed via the proper tuning of parameters and in the case of the implementation described here, the performance is better than the linear models that used the same variables.

Knowledge about specific fields related to the problem at hand is much valuable when creating hypothesis, defining models and transforming the data. If implementation expertise and final application of the solution can be combined the process of creation of models and interpretation of outputs will be more efficient.

```{r bib, include=FALSE, echo=FALSE}
# create a bib file for the R packages used in this document
knitr::write_bib(c('base', 'rmarkdown'), file = 'skeleton.bib')
```
